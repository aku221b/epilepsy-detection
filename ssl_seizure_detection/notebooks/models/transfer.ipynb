{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load Model\n",
    "Before we begin transfer learning we first have to load the model. This can be done in two ways (1) load the `model.pth` which includes the model's architecture and weights, or (2) load the model class itself, defined in `pyg_model.py`. Either way works, but (2) is a bit safer when dealing with unknown files. After loading the model, we then load the state dictionary `model_state_dict.pth` which allows us to reference specific layers of the model and is crucial for examining, extracting, or modifying its underlying architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VICRegT1(\n",
       "  (embedder): gnn_embedder2(\n",
       "    (edge_mlp): EdgeMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=576, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (conv1): NNConv(9, 64, aggr=add, nn=EdgeMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=576, bias=True)\n",
       "      )\n",
       "    ))\n",
       "    (conv2): GATConv(64, 128, heads=1)\n",
       "    (conv3): GATConv(128, 128, heads=1)\n",
       "    (net_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (bn_graph1): BatchNorm(64)\n",
       "    (bn_graph2): BatchNorm(128)\n",
       "    (bn_graph3): BatchNorm(128)\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# PC\n",
    "model_name = \"VICRegT1\"\n",
    "model_path = rf\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\test\\model\\{model_name}.pth\"\n",
    "model_dict_path = rf\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\test\\model\\{model_name}_state_dict.pth\"\n",
    "\n",
    "# Load model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "# Load state dictionary\n",
    "model_dict = torch.load(model_dict_path)\n",
    "\n",
    "# Set the state dictionary to the model\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Extract Layers\n",
    "In this step we extract the layers we want to use for the supervised model downstream. In this case, we need the NNConv and GATConv layers from our model, but since our NNConv actually depends on a separate layer called EdgeMLP (which is just a multilayer perpcetron), we'll need that too, since it's essentially part of the NNConv layer's parameters. You can assign it the old fashioned way using `EdgeMLP_module = model.edge_mlp` but this will create issues later on when we try to make two copies of `EdgeMLP_module` for freezing and unfreezing it, so we use the `copy` package instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mlp = copy.deepcopy(model.embedder.edge_mlp)\n",
    "conv1 = copy.deepcopy(model.embedder.conv1)\n",
    "conv2 = copy.deepcopy(model.embedder.conv2)\n",
    "conv3 = copy.deepcopy(model.embedder.conv3)\n",
    "bn_graph1 = copy.deepcopy(model.embedder.bn_graph1)\n",
    "bn_graph2 = copy.deepcopy(model.embedder.bn_graph2)\n",
    "bn_graph3 = copy.deepcopy(model.embedder.bn_graph3)\n",
    "bn1 = copy.deepcopy(model.embedder.bn1)\n",
    "bn2 = copy.deepcopy(model.embedder.bn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the weights of a layer with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.0.weight \t torch.Size([128, 3])\n",
      "mlp.0.bias \t torch.Size([128])\n",
      "mlp.2.weight \t torch.Size([64, 128])\n",
      "mlp.2.bias \t torch.Size([64])\n",
      "mlp.4.weight \t torch.Size([576, 64])\n",
      "mlp.4.bias \t torch.Size([576])\n"
     ]
    }
   ],
   "source": [
    "from models import set_requires_grad\n",
    "\n",
    "for param_tensor in edge_mlp.state_dict():\n",
    "    print(param_tensor, \"\\t\", edge_mlp.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a test running some random input through the EdgeMLP, to verify it's functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 90.8198, 166.7563, -65.1881,  ...,  33.9478,  27.4430, -26.9420],\n",
      "        [ 51.7824,  96.8568, -40.4229,  ...,  19.8774,  12.1248, -11.4472],\n",
      "        [ 64.5977, 119.4870, -45.8949,  ...,  25.7579,  21.9166, -21.7708],\n",
      "        ...,\n",
      "        [ 11.1861,  28.3259,  -8.5658,  ...,   9.3842,  10.6208,  -6.5623],\n",
      "        [ 90.4008, 165.8797, -62.6314,  ...,  34.0480,  30.7454, -30.2114],\n",
      "        [ -8.4273,  21.2802,  -8.8721,  ...,  21.9474,  16.3903,  -1.0682]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create some dummy data\n",
    "dummy_edge_attr = torch.randn(10, 3)  # 10 edges, each with `num_edge_features` features\n",
    "\n",
    "# Ensure everything is on the same device\n",
    "device = \"cuda\"\n",
    "dummy_edge_attr = dummy_edge_attr.to(device)\n",
    "edge_mlp = edge_mlp.to(device)\n",
    "\n",
    "# Run the data through the `edge_mlp` layer\n",
    "output = edge_mlp(dummy_edge_attr)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Downstream Task\n",
    "After extracting the layers and verifying everything is functional, we can now either (1) use the layers and their weights as initialization, or (2) use the layers but freeze the weights (i.e. they won't be updated during training). Below uses method (1), using our transferred layers as the initial layers of our network, and then we add on newer (untrained) layers on top of it. I've opted to use another `NNConv` and `GATConv` layer from `PyG`, adding onto the existing `NNConv` and `GATConv` layers, as well as a `global_mean_pool` layer and two fully connected layers. Now we're ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import downstream3\n",
    "\n",
    "pretrained_layers = {\"edge_mlp\": edge_mlp,\n",
    "                        \"conv1\": conv1,\n",
    "                        \"conv2\": conv2,\n",
    "                        \"conv3\": conv3,\n",
    "                        \"bn_graph1\": bn_graph1,\n",
    "                        \"bn_graph2\": bn_graph2,\n",
    "                        \"bn_graph3\": bn_graph3,\n",
    "                        }\n",
    "\n",
    "\n",
    "config = {\"classify\": \"multiclass\", \"head\": \"linear\"}\n",
    "\n",
    "model = downstream3(config, pretrained_layers=pretrained_layers, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check wehther the pretrained layers are frozen or not with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Encoder:\n",
      "Layer: conv1.bias, Frozen: True\n",
      "Layer: conv1.nn.mlp.0.weight, Frozen: True\n",
      "Layer: conv1.nn.mlp.0.bias, Frozen: True\n",
      "Layer: conv1.nn.mlp.2.weight, Frozen: True\n",
      "Layer: conv1.nn.mlp.2.bias, Frozen: True\n",
      "Layer: conv1.nn.mlp.4.weight, Frozen: True\n",
      "Layer: conv1.nn.mlp.4.bias, Frozen: True\n",
      "Layer: conv1.lin.weight, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.0.weight, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.0.bias, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.2.weight, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.2.bias, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.4.weight, Frozen: True\n",
      "Layer: conv1.edge_mlp.mlp.4.bias, Frozen: True\n",
      "Layer: conv2.att_src, Frozen: True\n",
      "Layer: conv2.att_dst, Frozen: True\n",
      "Layer: conv2.bias, Frozen: True\n",
      "Layer: conv2.lin_src.weight, Frozen: True\n",
      "Layer: conv3.att_src, Frozen: True\n",
      "Layer: conv3.att_dst, Frozen: True\n",
      "Layer: conv3.bias, Frozen: True\n",
      "Layer: conv3.lin_src.weight, Frozen: True\n",
      "Layer: bn_graph1.module.weight, Frozen: True\n",
      "Layer: bn_graph1.module.bias, Frozen: True\n",
      "Layer: bn_graph2.module.weight, Frozen: True\n",
      "Layer: bn_graph2.module.bias, Frozen: True\n",
      "Layer: bn_graph3.module.weight, Frozen: True\n",
      "Layer: bn_graph3.module.bias, Frozen: True\n",
      "\n",
      "Checking Classifier:\n",
      "Layer: fc1.weight, Frozen: False\n",
      "Layer: fc1.bias, Frozen: False\n",
      "Layer: fc2.weight, Frozen: False\n",
      "Layer: fc2.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "def check_frozen_status(model):\n",
    "    # Checking the encoder\n",
    "    print(\"Checking Encoder:\")\n",
    "    for name, param in model.encoder.named_parameters():\n",
    "        print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "    # Checking the classifier\n",
    "    print(\"\\nChecking Classifier:\")\n",
    "    for name, param in model.classifier.named_parameters():\n",
    "        print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "# Assuming 'model' is an instance of 'downstream3'\n",
    "\n",
    "config = {\"classify\": \"multiclass\", \"head\": \"softmax\"}\n",
    "model = downstream3(config, pretrained_layers=pretrained_layers, requires_grad=False).to(device)\n",
    "\n",
    "# Check if the encoder and classifier layers are frozen or not\n",
    "check_frozen_status(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning on Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in dataset: 2307.\n",
      "Total number of examples used: 2307.\n",
      "Number of training examples: 461. Number of training batches: 15.\n",
      "Number of validation examples: 461. Number of validation batches: 15.\n",
      "Number of test examples: 230. Number of test batches: 8.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../src\")\n",
    "from preprocess import create_data_loaders\n",
    "\n",
    "# PC\n",
    "data_path = r\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\jh101\\supervised\\jh101_combined.pt\"\n",
    "data = torch.load(data_path)\n",
    "\n",
    "loaders, _ = create_data_loaders(data, val_ratio=0.2, test_ratio=0.1, batch_size=32, num_workers=4, model_id=\"downstream3\", train_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([32, 3])\n",
      "Model output: tensor([[1.6486e-04, 7.0911e-05, 9.9976e-01],\n",
      "        [4.8101e-06, 2.1530e-04, 9.9978e-01],\n",
      "        [1.4486e-08, 1.2311e-06, 1.0000e+00],\n",
      "        [2.5720e-01, 6.4257e-02, 6.7855e-01],\n",
      "        [6.1876e-06, 2.4727e-04, 9.9975e-01],\n",
      "        [4.1747e-07, 2.3232e-05, 9.9998e-01],\n",
      "        [8.0231e-03, 2.0813e-02, 9.7116e-01],\n",
      "        [2.2811e-08, 1.7397e-08, 1.0000e+00],\n",
      "        [1.7307e-08, 1.9248e-07, 1.0000e+00],\n",
      "        [2.0880e-09, 2.3274e-08, 1.0000e+00],\n",
      "        [1.1480e-09, 3.4492e-08, 1.0000e+00],\n",
      "        [8.6305e-07, 3.7996e-06, 1.0000e+00],\n",
      "        [5.6311e-07, 3.5634e-08, 1.0000e+00],\n",
      "        [3.1421e-15, 5.4503e-15, 1.0000e+00],\n",
      "        [1.4451e-02, 3.9844e-02, 9.4570e-01],\n",
      "        [2.5450e-03, 2.0745e-02, 9.7671e-01],\n",
      "        [1.3459e-02, 4.0130e-06, 9.8654e-01],\n",
      "        [3.4352e-07, 2.1390e-06, 1.0000e+00],\n",
      "        [3.1483e-09, 1.3756e-07, 1.0000e+00],\n",
      "        [4.2724e-07, 1.8677e-05, 9.9998e-01],\n",
      "        [2.6758e-04, 1.6453e-07, 9.9973e-01],\n",
      "        [2.3466e-04, 6.1181e-03, 9.9365e-01],\n",
      "        [5.1792e-05, 3.7694e-04, 9.9957e-01],\n",
      "        [3.8962e-05, 6.5759e-04, 9.9930e-01],\n",
      "        [1.8534e-02, 4.2408e-02, 9.3906e-01],\n",
      "        [1.5659e-16, 1.7397e-16, 1.0000e+00],\n",
      "        [9.7247e-04, 6.6852e-03, 9.9234e-01],\n",
      "        [1.8361e-02, 3.7143e-02, 9.4450e-01],\n",
      "        [2.1892e-09, 9.5256e-08, 1.0000e+00],\n",
      "        [1.2909e-06, 2.3660e-05, 9.9998e-01],\n",
      "        [6.3387e-04, 5.1539e-03, 9.9421e-01],\n",
      "        [1.5557e-10, 2.4291e-08, 1.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = loaders\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    output = model(batch)\n",
    "    row_sums = torch.sum(output, dim=1)\n",
    "    print(f\"Output size: {output.size()}\") # We should expect a size of [batch_size, 3]\n",
    "    print(f\"Model output: {output}\") \n",
    "    print(f\"Row sums: {row_sums}\") # We should expect the row sums to be 1, by the softmax head\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Transfer Learning\n",
    "If you want to do all of the above in one step, see below. Note that this implemented in `train.py` when you select the `downstream1` or `downstream2` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from preprocess import extract_layers\n",
    "from models import downstream3\n",
    "\n",
    "# PC\n",
    "transfer_id = \"VICRegT1\"\n",
    "model_path = rf\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\test\\model\\{transfer_id}.pth\"\n",
    "model_dict_path = rf\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\test\\model\\{transfer_id}_state_dict.pth\"\n",
    "\n",
    "# Extract pretrained layers\n",
    "extracted_layers = extract_layers(model_path, model_dict_path, transfer_id)\n",
    "\n",
    "config = {\n",
    "    \"classify\": \"multiclass\",\n",
    "    \"head\": \"softmax\",\n",
    "}\n",
    "\n",
    "# Create downstream model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = downstream3(config, pretrained_layers=extracted_layers, requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "from preprocess import create_data_loaders\n",
    "\n",
    "# Mac\n",
    "# data_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/patient_pyg/jh101/supervised/jh101_run1.pt\"\n",
    "\n",
    "# PC\n",
    "data_path = r\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\jh101\\supervised\\jh101_run1.pt\"\n",
    "data = torch.load(data_path)\n",
    "loaders, _ = create_data_loaders(data, val_ratio=0.2, test_ratio=0.1, batch_size=32, num_workers=4, model_id=\"supervised\")\n",
    "train_loader = loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the model output is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: tensor([[6.9798e-01, 7.7237e-03, 2.9430e-01],\n",
      "        [4.7733e-01, 2.4406e-01, 2.7862e-01],\n",
      "        [6.1853e-01, 3.6772e-02, 3.4470e-01],\n",
      "        [8.0432e-01, 1.8714e-02, 1.7696e-01],\n",
      "        [6.0336e-01, 1.8149e-01, 2.1514e-01],\n",
      "        [9.1654e-01, 2.0765e-02, 6.2692e-02],\n",
      "        [2.1562e-01, 1.7578e-01, 6.0861e-01],\n",
      "        [2.0229e-01, 5.4471e-02, 7.4324e-01],\n",
      "        [3.5535e-03, 3.0523e-03, 9.9339e-01],\n",
      "        [6.3338e-03, 3.1206e-03, 9.9055e-01],\n",
      "        [2.5598e-01, 1.8944e-01, 5.5458e-01],\n",
      "        [6.1665e-01, 5.4741e-02, 3.2861e-01],\n",
      "        [5.9272e-01, 3.2995e-02, 3.7429e-01],\n",
      "        [5.6594e-01, 1.9310e-01, 2.4096e-01],\n",
      "        [5.1703e-01, 1.6244e-01, 3.2053e-01],\n",
      "        [9.3663e-01, 3.0902e-03, 6.0276e-02],\n",
      "        [7.8324e-01, 3.6755e-02, 1.8000e-01],\n",
      "        [7.9359e-01, 1.8001e-02, 1.8841e-01],\n",
      "        [4.6237e-01, 4.4573e-02, 4.9305e-01],\n",
      "        [5.6424e-01, 1.1708e-01, 3.1868e-01],\n",
      "        [7.3546e-01, 7.2142e-03, 2.5733e-01],\n",
      "        [7.2357e-01, 2.0231e-02, 2.5620e-01],\n",
      "        [8.3413e-01, 1.6139e-02, 1.4973e-01],\n",
      "        [9.4145e-01, 1.8183e-03, 5.6730e-02],\n",
      "        [9.2236e-01, 2.1383e-03, 7.5506e-02],\n",
      "        [7.4039e-01, 1.1929e-02, 2.4768e-01],\n",
      "        [7.6437e-02, 1.8058e-03, 9.2176e-01],\n",
      "        [4.4781e-05, 5.2397e-06, 9.9995e-01],\n",
      "        [1.6339e-03, 3.9683e-04, 9.9797e-01],\n",
      "        [1.0099e-02, 1.3508e-03, 9.8855e-01],\n",
      "        [7.2076e-02, 1.6019e-03, 9.2632e-01],\n",
      "        [9.1920e-06, 2.0607e-05, 9.9997e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    print(f\"Model Output: {model(batch)}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_cuda11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
